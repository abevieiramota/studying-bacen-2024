---
ano: 2017
tipo: paper
---
* **Abstract**: ~uso de recurrent/convolutional + attention -> transformer faz uso apenas de attention mechanism, conseguindo maior paralelismo (recurrent/convolutional fazem referência a estados anteriores)
* attention mechanism permite modelar dependências entre elementos na sequência
* **Transformer**: arquitetura que faz uso apenas de mecanismos de atenção, sem uso de 'recurrence'
* ~outros modelos similares têm custo de treinamento função da distância entre os pontos relacionados